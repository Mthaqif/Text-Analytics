{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c32f14a-4df6-401e-89ef-166495f3cfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                      Cleaned_Review  \n",
      "0               im happy uniten actually even people  \n",
      "1                 pretty good time happy meet people  \n",
      "2                      neutral place term everything  \n",
      "3  would say uniten good university issue need im...  \n",
      "4  uniten wellregarded particularly strong engine...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/d754b29c-508d-4bde-938d-\n",
      "[nltk_data]     34c5de7c5059/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# -----------------------------\n",
    "# Download Required Resources\n",
    "# -----------------------------\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Tools\n",
    "# -----------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Keep negation words (important for sentiment analysis)\n",
    "for word in [\"not\", \"no\", \"nor\"]:\n",
    "    if word in stop_words:\n",
    "        stop_words.remove(word)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# -----------------------------\n",
    "# Contractions Dictionary\n",
    "# -----------------------------\n",
    "contractions_dict = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"we're\": \"we are\"\n",
    "}\n",
    "\n",
    "escaped_contractions = [re.escape(c) for c in contractions_dict.keys()]\n",
    "pattern = r'\\b(' + '|'.join(escaped_contractions) + r')\\b'\n",
    "compiled_pattern = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# -----------------------------\n",
    "# Cleaning Functions\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_apostrophes(text):\n",
    "    # Fix curly quotes\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\n",
    "    # Fix spaced forms like i ’ m → i'm\n",
    "    text = re.sub(r\"\\b(\\w)\\s*'\\s*(\\w)\\b\", r\"\\1'\\2\", text)\n",
    "    return text\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace_match(match):\n",
    "        return contractions_dict[match.group(0).lower()]\n",
    "    return compiled_pattern.sub(replace_match, text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "def remove_single_letters(text):\n",
    "    return re.sub(r'\\b[a-z]\\b', '', text)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text, preserve_line=True)\n",
    "    pos_tags = pos_tag(words, lang='eng')\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "    ]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "# -----------------------------\n",
    "# Full Preprocessing Pipeline\n",
    "# -----------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = normalize_apostrophes(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_single_letters(text)\n",
    "    text = lemmatize_text(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"Cleaned_Review\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"UNITENReview_Exercise.csv\", index=False)\n",
    "\n",
    "# Preview result\n",
    "print(df[[\"Review\", \"Cleaned_Review\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d9bc5-d9a3-4fba-a622-2297053a7dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7566e-37e5-472d-b40e-00c6f26d4646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
